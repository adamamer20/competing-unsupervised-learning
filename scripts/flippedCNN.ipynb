{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## CNN with competing hidden filters: unsupervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In this notebook, we build a CNN where learning happens exclusively in an unsupervised manner and filters are pushed to compete against one another(similar to the competition among hidden units in the paper \"Unsupervised learning by competing hidden units\" by Krotov and Hopfield). The rationale behind the model is that each filter should specialize in detecting unique features, enhancing diversity and representation; to do so, we orthogonalize the filters such that there is minimal overlapping over each filter and, thus, there's maximum specialization of each filter for a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The CNN is built with PyTorch, using the Adam optimizer and a learning rate of 0.01. Since the classification problem at hand is relatively simple, we have opted for a simpler model with a low number of layers to avoid overfitting. Futhermore, empirical testing has shown diminishing returns in more complex models so, having limited resources, we decided to stick to a simpler CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color='red'> TO DO:\n",
    "\n",
    " 1. TEST SGD\n",
    "\n",
    " 2. TEST A SMALLER LEARNING RATE OF 1e-4 FOR ADAM\n",
    "\n",
    " 3. TRY ADDING ONE LAYER FOR CIFAR (since the problem is slightly more complex)\n",
    "\n",
    " 4. TEST MORE COMPLEX ORTHOGONALITY\n",
    "\n",
    " 5. TRY ADDING BIAS\n",
    "\n",
    " 6. TRY K-TOP FILTERS AND NOT ONLY ONE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color='green'> COMPLETED:\n",
    "\n",
    " 1. Adam with learning rate 0.01 --> 2 unsupervised epochs are enough on MNIST (92.54%), 5 unsupervised epochs are not enough on CIFAR (50.64%)\n",
    "\n",
    " 2. Gramh-Schmidt diversification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from optuna.trial import Trial\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Filters(nn.Module):\n",
    "    \"\"\"\n",
    "    Below, we have a custom layer that doesn't rely on backpropagation for training. Instead, it uses:\n",
    "        1) a forward pass to takes an input to get the filters' responses.\n",
    "        2) A custom update rule (e.g., winner-take-most \"WTA\") to update filters.\n",
    "        3) An orthonormalization step to ensure diversity among filters.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        inputChannels,\n",
    "        outputChannels,\n",
    "        kernel=3,\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        delta=0.1,\n",
    "        k=5,\n",
    "    ):\n",
    "        super(Filters, self).__init__()\n",
    "        self.inputChannels = inputChannels  # e.g., 3 for CIFAR-10 since the images are RGB and 1 for MNIST\n",
    "        self.outputChannels = outputChannels\n",
    "        self.kernel = kernel\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.delta = delta\n",
    "        self.k = k\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "        # we initialize weights manually to have more control\n",
    "        # we sample from a normal distribution and multiply by 0.01 to have small weights at the start (to prevent large outputs at the beginning)\n",
    "        # requires_grad = False explicitly indicates that we have our own custom update rule, defined later on\n",
    "        weightShape = (outputChannels, inputChannels, kernel, kernel)\n",
    "        self.weights = nn.Parameter(\n",
    "            torch.randn(weightShape) * 0.01, requires_grad=False\n",
    "        )\n",
    "        self.bias = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # we use a standard forward pass\n",
    "        return nn.functional.conv2d(\n",
    "            x, self.weights, bias=self.bias, stride=self.stride, padding=self.padding\n",
    "        )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, x, learningRate=0.01, competitionMode=\"wta\"):\n",
    "        \"\"\"\n",
    "        Modified unsupervised update rule:\n",
    "        1) Forward pass -> compute 'response'\n",
    "        2) For each patch:\n",
    "             - The top filter is updated positively (winner-take-all).\n",
    "             - The next 'k' filters are pushed away (negative update).\n",
    "        3) We accumulate updates in a tensor (using 'index_add_').\n",
    "        4) Finally, we add those updates into 'self.weights' with the appropriate sign.\n",
    "        \"\"\"\n",
    "        # 1) Forward pass & find top (k+1) filters\n",
    "        #    response: [B, c_out, H_out, W_out]\n",
    "        response = self.forward(x)\n",
    "\n",
    "        # topk_vals: [B, k+1, H_out, W_out]\n",
    "        # topk_idxs: [B, k+1, H_out, W_out] -- these are the filter indices\n",
    "        topk_vals, topk_idxs = response.topk(self.k + 1, dim=1)\n",
    "\n",
    "        # The \"winner\" is the 0th in that list\n",
    "        # The next 'k' are the runner-ups we want to push away\n",
    "        winner_idxs = topk_idxs[:, 0, :, :]  # shape [B, H_out, W_out]\n",
    "        losers_idxs = topk_idxs[:, 1:, :, :]  # shape [B, k, H_out, W_out]\n",
    "\n",
    "        # 2) Unfold input into patches\n",
    "        #    x_unfolded: [B, c_in*kernel*kernel, num_patches]\n",
    "        x_unfolded = F.unfold(\n",
    "            x, kernel_size=self.kernel, padding=self.padding, stride=self.stride\n",
    "        )\n",
    "        B, _, num_patches = x_unfolded.shape\n",
    "\n",
    "        # Flatten out the (H_out * W_out) dimension for indexing\n",
    "        # winner_flat[b] -> shape [num_patches] for batch b\n",
    "        winner_flat = winner_idxs.reshape(B, -1)\n",
    "        # losers_flat[b] -> shape [k, num_patches] for batch b\n",
    "        losers_flat = losers_idxs.reshape(B, self.k, -1)\n",
    "\n",
    "        # 3) Create accumulators for winners and losers\n",
    "        accumulation_winner = torch.zeros(\n",
    "            (self.outputChannels, self.inputChannels * self.kernel * self.kernel),\n",
    "            device=self.device,\n",
    "        )\n",
    "        accumulation_losers = torch.zeros_like(accumulation_winner)\n",
    "\n",
    "        # 4) Accumulate updates (index_add_) for each batch\n",
    "        #\n",
    "        # Winners: + learningRate * x_unfolded[b].T\n",
    "        # Losers:  - delta * x_unfolded[b].T\n",
    "        #\n",
    "        # We do a for-loop over B for clarity, and a nested loop over k\n",
    "        # for the losers. The patch dimension is handled by index_add_ internally.\n",
    "        for b in range(B):\n",
    "            # 4a) winner update\n",
    "            accumulation_winner.index_add_(\n",
    "                dim=0,\n",
    "                index=winner_flat[b],  # which filters to update\n",
    "                source=x_unfolded[b].T,  # shape [num_patches, c_in*k*k]\n",
    "            )\n",
    "            # 4b) losers update\n",
    "            #     We do a small loop for each of the k losers.\n",
    "            #     Each row in losers_flat[b] is shape [num_patches].\n",
    "            for i in range(self.k):\n",
    "                accumulation_losers.index_add_(\n",
    "                    dim=0, index=losers_flat[b, i], source=x_unfolded[b].T\n",
    "                )\n",
    "\n",
    "        # 5) Perform the actual weight update\n",
    "        #    weights += learningRate * (accumulation_winner - delta * accumulation_losers)\n",
    "        self.weights += learningRate * (\n",
    "            accumulation_winner.view_as(self.weights)\n",
    "            - self.delta * accumulation_losers.view_as(self.weights)\n",
    "        )\n",
    "\n",
    "    def orthonormalization(self, error=1e-8):\n",
    "        \"\"\"\n",
    "        Here, we orthonormalize the filters using Gram-Schmidt. Each filter has shape [inputChannels * kernel^2]\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # we reshape the filters in order to be able to apply Gram-Schmidt\n",
    "            flattenedWeights = self.weights.view(self.outputChannels, -1)\n",
    "            matrix = []\n",
    "            for i in range(self.outputChannels):\n",
    "                # we take the current filter's weights...\n",
    "                current = flattenedWeights[i].clone()\n",
    "                # ... and subtract projections of current on already processed filters in the matrix\n",
    "                for j in matrix:\n",
    "                    projection = (current @ j) * j\n",
    "                    current -= projection\n",
    "\n",
    "                # we normalize the resulting vector to have a unit norm\n",
    "                norm = torch.norm(current) + error\n",
    "                current = current / norm\n",
    "                matrix.append(current)\n",
    "\n",
    "            # we convert the list of 1d vectors to a 2d tensor and then reshape the tensor to the original 4d shape\n",
    "            matrix = torch.stack(matrix, dim=0)\n",
    "            self.weights.data = matrix.view_as(self.weights.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompetingCNN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inputChannels=3,\n",
    "        num_classes=10,\n",
    "        feature_dim=64,\n",
    "        kernel=3,\n",
    "        stride=1,\n",
    "        padding=1,\n",
    "        pool_size=4,\n",
    "        delta=0.1,\n",
    "        k=5,\n",
    "    ):\n",
    "        super(CompetingCNN, self).__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.to(self.device)\n",
    "\n",
    "        self.conv1 = Filters(\n",
    "            inputChannels=inputChannels,\n",
    "            outputChannels=feature_dim,\n",
    "            kernel=kernel,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            delta=delta,\n",
    "            k=k,\n",
    "        )\n",
    "        self.conv2 = Filters(\n",
    "            inputChannels=feature_dim,\n",
    "            outputChannels=feature_dim,\n",
    "            kernel=kernel,\n",
    "            stride=stride,\n",
    "            padding=padding,\n",
    "            delta=delta,\n",
    "            k=k,\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d((pool_size, pool_size))\n",
    "        self.classifier = nn.Linear(feature_dim * pool_size * pool_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # we follow both convolutions with a ReLU to add non-linearity to learn more complex patterns\n",
    "        x = self.conv1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        output = self.classifier(x)\n",
    "        return output\n",
    "\n",
    "    def update(self, x, learningRate=0.01, competitionMode=\"wta\"):\n",
    "        \"\"\"\n",
    "        Here, we call the unsupervised update rule of each convolution layer. Then, we normalize.\n",
    "        \"\"\"\n",
    "        # update filters in conv1\n",
    "        self.conv1.update(x, learningRate=learningRate, competitionMode=competitionMode)\n",
    "        self.conv1.orthonormalization()\n",
    "\n",
    "        # forward pass from conv1\n",
    "        with torch.no_grad():\n",
    "            out1 = self.conv1.forward(x)\n",
    "            out1 = nn.functional.relu(out1)\n",
    "\n",
    "        # update filters in conv2 based on output of conv1\n",
    "        self.conv2.update(\n",
    "            out1, learningRate=learningRate, competitionMode=competitionMode\n",
    "        )\n",
    "        self.conv2.orthonormalization()\n",
    "\n",
    "    def freezeLayers(self, freeze=True):\n",
    "        \"\"\"\n",
    "        Here, we freeze the weights of the unsupervised convolutional layers.\n",
    "        \"\"\"\n",
    "        for i in self.conv1.parameters():\n",
    "            i.requires_grad = not freeze\n",
    "\n",
    "        for i in self.conv2.parameters():\n",
    "            i.requires_grad = not freeze\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_validation_accuracy(model, val_loader, device):\n",
    "    \"\"\"Helper function to compute validation accuracy\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100.0 * correct / total\n",
    "\n",
    "\n",
    "def compute_improvement_rate(accuracies):\n",
    "    \"\"\"Helper function to compute average improvement rate\"\"\"\n",
    "    if len(accuracies) < 2:\n",
    "        return accuracies[0]\n",
    "    improvements = [b - a for a, b in zip(accuracies[:-1], accuracies[1:])]\n",
    "    return sum(improvements) / len(improvements)\n",
    "\n",
    "\n",
    "def trainCNN(\n",
    "    dataset_name=\"MNIST\",\n",
    "    batch_size=64,\n",
    "    unsupervised_epochs=5,\n",
    "    supervised_epochs=5,\n",
    "    unsupervised_learning_rate=0.01,\n",
    "    supervised_learning_rate=0.001,\n",
    "    competition_mode=\"wta\",\n",
    "    feature_dim=64,\n",
    "    kernel=3,\n",
    "    stride=1,\n",
    "    padding=1,\n",
    "    pool_size=4,\n",
    "    delta=0.1,\n",
    "    k=5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Here, we train a CNN using the CompetingCNN model\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if dataset_name.lower() == \"mnist\":\n",
    "        # we normalize the data to have zero mean and unit variance\n",
    "        pipeline = transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        )\n",
    "        trainingData = datasets.MNIST(\n",
    "            root=\"./data\", train=True, transform=pipeline, download=True\n",
    "        )\n",
    "        testingData = datasets.MNIST(\n",
    "            root=\"./data\", train=False, transform=pipeline, download=True\n",
    "        )\n",
    "        inputChannels = 1  # MNIST is grayscale, thus the input channel is 1\n",
    "        num_classes = 10\n",
    "    elif dataset_name.lower() == \"fashionmnist\":\n",
    "        pipeline = transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.2860,), (0.3530,))]\n",
    "        )\n",
    "        trainingData = datasets.FashionMNIST(\n",
    "            root=\"./data\", train=True, transform=pipeline, download=True\n",
    "        )\n",
    "        testingData = datasets.FashionMNIST(\n",
    "            root=\"./data\", train=False, transform=pipeline, download=True\n",
    "        )\n",
    "        inputChannels = 1  # FashionMNIST is grayscale\n",
    "        num_classes = 10\n",
    "    elif dataset_name.lower() == \"cifar100\":\n",
    "        pipeline = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    (0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        trainingData = datasets.CIFAR100(\n",
    "            root=\"./data\", train=True, transform=pipeline, download=True\n",
    "        )\n",
    "        testingData = datasets.CIFAR100(\n",
    "            root=\"./data\", train=False, transform=pipeline, download=True\n",
    "        )\n",
    "        inputChannels = 3\n",
    "        num_classes = 100\n",
    "    elif dataset_name.lower() == \"cifar10\":\n",
    "        pipeline = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    (0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        trainingData = datasets.CIFAR10(\n",
    "            root=\"./data\", train=True, transform=pipeline, download=True\n",
    "        )\n",
    "        testingData = datasets.CIFAR10(\n",
    "            root=\"./data\", train=False, transform=pipeline, download=True\n",
    "        )\n",
    "        inputChannels = 3\n",
    "        num_classes = 10\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Unsupported dataset. Use 'MNIST', 'FashionMNIST', 'CIFAR10', or 'CIFAR100'\"\n",
    "        )\n",
    "\n",
    "    # Split training data into train and validation sets\n",
    "    validation_split = 0.1\n",
    "    train_size = int((1 - validation_split) * len(trainingData))\n",
    "    val_size = len(trainingData) - train_size\n",
    "\n",
    "    train_dataset, val_dataset = random_split(\n",
    "        trainingData,\n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(42),\n",
    "    )\n",
    "\n",
    "    train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test = DataLoader(testingData, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # we initialize the model and save it\n",
    "    model = CompetingCNN(\n",
    "        inputChannels=inputChannels,\n",
    "        num_classes=num_classes,\n",
    "        feature_dim=feature_dim,\n",
    "        kernel=kernel,\n",
    "        stride=stride,\n",
    "        padding=padding,\n",
    "        pool_size=pool_size,\n",
    "        delta=delta,\n",
    "        k=k,\n",
    "    ).to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Lists to store validation accuracies\n",
    "    unsupervised_val_accuracies = []\n",
    "    supervised_val_accuracies = []\n",
    "\n",
    "    # unsupervised pretraining using the custom update rule\n",
    "    print(\"STARTING UNSUPERVISED PRE-TRAINING\")\n",
    "    for epoch in tqdm(range(unsupervised_epochs)):\n",
    "        for batchIndex, (images, unusedLabels) in enumerate(train):\n",
    "            images = images.to(device)\n",
    "            model.update(\n",
    "                images,\n",
    "                learningRate=unsupervised_learning_rate,\n",
    "                competitionMode=competition_mode,\n",
    "            )\n",
    "\n",
    "        # Check validation accuracy every 2 epochs\n",
    "        if epoch % 2 == 0:\n",
    "            val_acc = compute_validation_accuracy(model, val, device)\n",
    "            unsupervised_val_accuracies.append(val_acc)\n",
    "            print(\n",
    "                f\"Unsupervised Epoch [{epoch + 1}/{unsupervised_epochs}], Validation Accuracy: {val_acc:.2f}%\"\n",
    "            )\n",
    "\n",
    "    # we freeze weights to avoid learning continues after the unsupervised part\n",
    "    print(\"FREEZING WEIGHTS\")\n",
    "    model.freezeLayers(freeze=True)\n",
    "\n",
    "    # supervised training with frozen weights\n",
    "    print(\"STARTING SUPERVISED TRAINING\")\n",
    "    optimizer = optim.Adam(model.classifier.parameters(), lr=supervised_learning_rate)\n",
    "    crossEntropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    supervised_loss_list = []\n",
    "    supervised_acc_list = []\n",
    "\n",
    "    for epoch in tqdm(range(supervised_epochs)):\n",
    "        model.train()\n",
    "        currentLoss = 0.0\n",
    "        epoch_correct = 0\n",
    "        epoch_samples = 0\n",
    "\n",
    "        for batchIndex, (images, labels) in enumerate(train):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = crossEntropy(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct = (predicted == labels).sum().item()\n",
    "            accuracy = correct / labels.size(0)\n",
    "            supervised_loss_list.append(loss.item())\n",
    "            supervised_acc_list.append(accuracy)\n",
    "\n",
    "            # 6) Accumulate for epoch-level average\n",
    "            currentLoss += loss.item() * labels.size(0)\n",
    "            epoch_correct += correct\n",
    "            epoch_samples += labels.size(0)\n",
    "\n",
    "        epoch_avg_loss = currentLoss / epoch_samples\n",
    "        epoch_avg_acc = epoch_correct / epoch_samples\n",
    "\n",
    "        print(\n",
    "            f\"Supervised Epoch [{epoch + 1}/{supervised_epochs}], \"\n",
    "            f\"Loss: {epoch_avg_loss:.4f}, \"\n",
    "            f\"Accuracy: {epoch_avg_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Check validation accuracy every 2 epochs\n",
    "        if epoch % 2 == 0:\n",
    "            val_acc = compute_validation_accuracy(model, val, device)\n",
    "            supervised_val_accuracies.append(val_acc)\n",
    "            print(\n",
    "                f\"Supervised Epoch [{epoch + 1}/{supervised_epochs}], Validation Accuracy: {val_acc:.2f}%\"\n",
    "            )\n",
    "\n",
    "    # Compute improvement rates\n",
    "    unsupervised_improvement = compute_improvement_rate(unsupervised_val_accuracies)\n",
    "    supervised_improvement = compute_improvement_rate(supervised_val_accuracies)\n",
    "    final_val_accuracy = (\n",
    "        supervised_val_accuracies[-1] if supervised_val_accuracies else 0.0\n",
    "    )\n",
    "\n",
    "    # evaluation on test set\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    acc = 100.0 * correct / total\n",
    "    print(f\"Test Accuracy: {acc:.2f}%\")\n",
    "\n",
    "    return (\n",
    "        model,\n",
    "        supervised_acc_list,\n",
    "        supervised_loss_list,\n",
    "        final_val_accuracy,\n",
    "        unsupervised_improvement,\n",
    "        supervised_improvement,\n",
    "    )\n",
    "\n",
    "\n",
    "def objective(trial: Trial, datasets: list) -> tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Multi-objective optimization function that evaluates across all datasets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    trial : Trial\n",
    "        Optuna trial object.\n",
    "    datasets : list\n",
    "        List of dataset names to evaluate.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        (Average Validation Accuracy, Average Unsupervised Improvement,\n",
    "         Average Supervised Improvement, Time Taken)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    params = {\n",
    "        \"batch_size\": trial.suggest_int(\"batch_size\", 32, 256),\n",
    "        \"unsupervised_epochs\": trial.suggest_int(\"unsupervised_epochs\", 1, 20),\n",
    "        \"supervised_epochs\": trial.suggest_int(\"supervised_epochs\", 1, 20),\n",
    "        \"unsupervised_learning_rate\": trial.suggest_float(\n",
    "            \"unsupervised_learning_rate\", 0.001, 0.8\n",
    "        ),\n",
    "        \"supervised_learning_rate\": trial.suggest_float(\n",
    "            \"supervised_learning_rate\", 1e-4, 1e-1, log=True\n",
    "        ),\n",
    "        \"feature_dim\": trial.suggest_int(\"feature_dim\", 32, 128),\n",
    "        \"kernel\": trial.suggest_int(\"kernel\", 3, 7, step=2),\n",
    "        \"stride\": trial.suggest_int(\"stride\", 1, 2),\n",
    "        \"padding\": trial.suggest_int(\"padding\", 0, 2),\n",
    "        \"pool_size\": trial.suggest_int(\"pool_size\", 2, 8),\n",
    "        \"delta\": trial.suggest_float(\"delta\", 0.05, 0.5),\n",
    "        \"k\": trial.suggest_int(\"k\", 1, 10),\n",
    "    }\n",
    "\n",
    "    total_val_accuracy = 0.0\n",
    "    total_unsup_improvement = 0.0\n",
    "    total_sup_improvement = 0.0\n",
    "    for dataset_name in datasets:\n",
    "        try:\n",
    "            _, _, _, val_accuracy, unsup_improvement, sup_improvement = trainCNN(\n",
    "                dataset_name=dataset_name, **params\n",
    "            )\n",
    "            total_val_accuracy += val_accuracy\n",
    "            total_unsup_improvement += unsup_improvement\n",
    "            total_sup_improvement += sup_improvement\n",
    "        except Exception as e:\n",
    "            print(f\"Error in dataset {dataset_name}: {e}\")\n",
    "            return float(\"-inf\"), float(\"-inf\"), float(\"-inf\"), float(\"inf\")\n",
    "\n",
    "    average_val_accuracy = total_val_accuracy / len(datasets)\n",
    "    average_unsup_improvement = total_unsup_improvement / len(datasets)\n",
    "    average_sup_improvement = total_sup_improvement / len(datasets)\n",
    "    time_taken = time.time() - start_time\n",
    "\n",
    "    return (\n",
    "        average_val_accuracy,\n",
    "        average_unsup_improvement,\n",
    "        average_sup_improvement,\n",
    "        time_taken,\n",
    "    )\n",
    "\n",
    "\n",
    "def optimize_hyperparameters(datasets: list, n_trials: int = 100) -> dict:\n",
    "    \"\"\"\n",
    "    Hyperparameter optimization across multiple datasets using Optuna.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datasets : list\n",
    "        List of dataset names to optimize on.\n",
    "    n_trials : int, optional\n",
    "        Number of trials for optimization, by default 100.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Best hyperparameters found.\n",
    "    \"\"\"\n",
    "    study = optuna.create_study(\n",
    "        directions=[\"maximize\", \"maximize\", \"maximize\", \"minimize\"],\n",
    "        study_name=\"CNN_multi_dataset_optimization\",\n",
    "        storage=\"sqlite:///CNN_multi_dataset_optimization.db\",\n",
    "    )\n",
    "    study.optimize(lambda trial: objective(trial, datasets), n_trials=n_trials)\n",
    "\n",
    "    print(\"\\nPareto Front:\")\n",
    "    for trial in study.best_trials:\n",
    "        print(\"\\nTrial:\")\n",
    "        print(f\"  Average Validation Accuracy: {trial.values[0]:.2f}%\")\n",
    "        print(f\"  Average Unsupervised Improvement: {trial.values[1]:.2f}%\")\n",
    "        print(f\"  Average Supervised Improvement: {trial.values[2]:.2f}%\")\n",
    "        print(f\"  Time Taken: {trial.values[3]:.2f} seconds\")\n",
    "        print(\"  Params:\")\n",
    "        for key, value in trial.params.items():\n",
    "            print(f\"    {key}: {value}\")\n",
    "\n",
    "    # Return the parameters of the trial with highest validation accuracy\n",
    "    best_accuracy_trial = max(study.best_trials, key=lambda t: t.values[0])\n",
    "    return best_accuracy_trial.params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    dataset_names = [\"FashionMNIST\", \"CIFAR100\", \"MNIST\"]\n",
    "    print(\n",
    "        \"\\nOptimizing hyperparameters across all datasets with multi-objective optimization\"\n",
    "    )\n",
    "    best_params = optimize_hyperparameters(dataset_names, n_trials=100)\n",
    "\n",
    "    for dataset in dataset_names:\n",
    "        print(f\"\\nTraining final model on {dataset} with best parameters\")\n",
    "        (\n",
    "            model,\n",
    "            loss_history,\n",
    "            acc_history,\n",
    "            val_accuracy,\n",
    "            unsup_improvement,\n",
    "            sup_improvement,\n",
    "        ) = trainCNN(dataset_name=dataset, **best_params)\n",
    "\n",
    "        print(f\"Final Validation Accuracy for {dataset}: {val_accuracy:.2f}%\")\n",
    "        print(f\"Unsupervised Improvement Rate for {dataset}: {unsup_improvement:.2f}%\")\n",
    "        print(f\"Supervised Improvement Rate for {dataset}: {sup_improvement:.2f}%\")\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(loss_history)\n",
    "        plt.title(f\"{dataset} Loss History\")\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(acc_history)\n",
    "        plt.title(f\"{dataset} Accuracy History\")\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
